Once that first blush fades, it becomes clear that ChatGPT doesn't actually
know anything--instead, it outputs compositions that simulate knowledge
through persuasive structure.
    --Ian Bogost, "Generative Art Is Stupid" (_The Atlantic,_ 2023-01-14)
%
ChatGPT is autocomplete on steroids. [paraphrased]
    --Gary Marcus
%
Q to ChatGPT: Where is New Scotland Yard?
A: New Scotland Yard is located at 8-10 Broadway, Westminster, London,
   England. It was the headquarters of the Metropolitan Police Service
   (MPS) from 1890 to 1967, and the name "New Scotland Yard" was also used
   as a metonym for the MPS itself. However, since 2017, the MPS moved to a
   new location, Metropolitan Police Service at Curtis Green, London,
   England.
(Actually, New Scotland Yard was at Embankment from 1890 to 1967, 8-10
Broadway from 1967 to 2017, and moved back to Embankment, to the Curtis
Green building adjacent to the previous headquarters, in 2017.)
%
Our mission is to ensure that artificial general intelligence benefits all
of humanity.
    --OpenAI.com, a company that does machine learning, not AGI
%
This [ChatGPT] kind of artificial intelligence we're talking about right
now can sometimes lead to something we call hallucination. This then
expresses itself in such a way that a machine provides a convincing but
completely made-up answer.
    --Prabhakar Raghavan, head of Google Search
%
Think of ChatGPT as a blurry jpeg of all the text on the Web. It retains
much of the information on the Web, in the same way that a jpeg retains
much of the information of a higher-resolution image, but, if you're
looking for an exact sequence of bits, you won't find it; all you will ever
get is an approximation. But, because the approximation is presented in the
form of grammatical text, which ChatGPT excels at creating, it's usually
acceptable. You're still looking at a blurry jpeg, but the blurriness
occurs in a way that doesn't make the picture as a whole look less sharp.
    --Ted Chiang, _The New Yorker,_ 2023-02-09
%
The fact that ChatGPT rephrases material from the Web instead of quoting it
word for word makes it seem like a student expressing ideas in her own
words, rather than simply regurgitating what she's read; it creates the
illusion that ChatGPT understands the material. In human students, rote
memorization isn't an indicator of genuine learning, so ChatGPT's inability
to produce exact quotes from Web pages is precisely what makes us think
that it has learned something. When we're dealing with sequences of words,
lossy compression looks smarter than lossless compression.
    --Ted Chiang, _The New Yorker,_ 2023-02-09
%
  Will letting a large language model handle the boilerplate allow writers to
focus their attention on the really creative parts?
  Obviously, no one can speak for all writers, but let me make the argument
that starting with a blurry copy of unoriginal work isn't a good way to
create original work....
   There's nothing magical or mystical about writing, but it involves more
than placing an existing document on an unreliable photocopier and pressing
the Print button.
    --Ted Chiang, _The New Yorker,_ 2023-02-09
%
You've probably already seen examples of chat Bots making simple mistakes
or getting things wrong, but perhaps more worrying are examples of them
confidently spouting false information, something which AI experts refer to
as hallucinating. One reporter asked a chatbot to "...write an essay
about... 'Belgian chemist and political philosopher Antoine De Machelet'"
(who does not exist by the way) and "...without hesitating the software
replied with a cogent well-organized bio populated entirely with imaginary
facts..."
  Basically these programs seem to be the George Santos of technology:
they're incredibly confident, incredibly dishonest, and for some reason
people seem to find that more amusing than dangerous.
    --Last Week Tonight with John Oliver,
      "Artificial Intelligence" https://youtu.be/Sqa8Zo2XWc4?t=1033
%
AI language models are notorious bullshitters, often presenting falsehoods
as facts.
    --Melissa Heikkilä, _MIT Technology Review_
%
Deep learning has absolutely nothing to do with intelligence as we observe
it in humans and animals. Why? Because it is inherently incapable of
effectively generalizing. Objective function optimization (the gradient
learning mechanism that LeCun is married to) is the opposite of
generalization. This is not a problem that can be fixed with add-ons. It's
a fundamental flaw in DL that makes it irrelevant to AGI.
%
  Large language models in particular are confident bullshitters: they
create text that sounds correct but in fact may be full of falsehoods....
  There is a more serious long-term implication. We may be witnessing, in
real time, the birth of a snowball of bullshit.
    --Melissa Heikkilä, "How AI-generated text is poisoning the Internet"
      _MIT Technology Review,_ 2022-12-20
%
Using the Peano axioms, you can prove that 1 + 1 = 2 as follows:
1. Base case: Show that 1 + 1 = 2 is true for 0....
2. Inductive step: Assume that 1 + 1 = 2 is true for some natural number n....
3. Conclusion: By the principle of mathematical induction, 1 + 1 = 2 is
   true for all natural numbers.
--Bing Chatbot (OpenGPT) asked to present an inductive proof that 1 + 1 = 2.
%
  The correct title for the third act of "The Sting" is "The Tale," and
the correct title for the third act is "The Shut Out."
  The five acts of "The Sting" and their corresponding intertitle cards
are:
  Act I: "The Set-Up"
  Act II: "The Hook"
  Act III: "The Tale"
  Act IV: "The Wire"
  Act V: "The Sting"
--OpenGPT
%
GPT-4 is 82% less likely to respond to requests for disallowed content and
40% more likely to produce factual responses than GPT-3.5 on our internal
evaluations.
    --OpenAI.com website, 2023-03-16
%
While ChatGPT is a sophisticated AI model, it is important to note that it
has been designed for a specific purpose, namely generating human-like text
based on input text.
    --ChatGPT Mar 14 Version
%
Distrust anyone who pretends that this is an echo, even an approximation,
of consciousness.
    --James Bridle, "The Stupidity of AI" (_The Guardian_ 2023-03-16)
%
The belief in this kind of AI as actually knowledgeable or meaningful is
actively dangerous. It risks poisoning the well of collective thought, and
of our ability to think at all. If, as is being proposed by technology
companies, the results of ChatGPT queries will be provided as answers to
those seeking knowledge online, and if, as has been proposed by some
commentators, ChatGPT is used in the classroom as a teaching aide, then its
hallucinations will enter the permanent record, effectively coming between
us and more legitimate, testable sources of information, until the line
between the two is so blurred as to be invisible. Moreover, there has never
been a time when our ability as individuals to research and critically
evaluate knowledge on our own behalf has been more necessary, not least
because of the damage that technology companies have already done to the
ways in which information is disseminated. To place all of our trust in the
dreams of badly programmed machines would be to abandon such critical
thinking altogether.
    --James Bridle, "The Stupidity of AI" (_The Guardian_ 2023-03-16)
%
The right way to think of the models that we create is a reasoning engine,
not a fact database.
    --Outright lie from Sam Altman, CEO of OpenAI, 2023-03-19
      https://abcnews.go.com/Technology/openai-ceo-sam-altman-ai-reshape-society-acknowledges/story?id=97897122
%
But why should it be startling in the slightest that software trained on
the entire text of the internet performs well on standardized exams? AI can
instantly run what amounts to an open-book test on any subject through
statistical analysis and regression. Indeed, that anyone is surprised at
all by this success suggests that people tend to get confused about what it
means when computers prove effective at human activities.
    --Ian Bogost, "Is This the Singularity for Standardized Tests?"
      _The Atlantic_ 2023-03-21
%
People want to believe so badly that these language models are actually
intelligent that they're willing to take themselves as a point of reference
and devalue that to match what the language model can do.
    --Emily Bender, responding to "People are just stochastic parrots"
%
In earlier days of neural nets, there tended to be the idea that one should
"make the neural net do as little as possible". For example, in converting
speech to text it was thought that one should first analyze the audio of
the speech, break it into phonemes, etc. But what was found is that--at
least for "human-like tasks"--it's usually better just to try to train the
neural net on the "end-to-end problem", letting it "discover" the necessary
intermediate features, encodings, etc. for itself.
    --Stephen Wolfram, "What Is ChatGPT Doing...and Why Does It Work?"
%
Large language models can be pretty bad at generating accurate facts and
knowledge. But they're pretty darn good at creating plausible renditions of
the work output you don't care that much about.
    --Ian Bogost, "The End of Recommendation Letters"
%
So [Huculak] asked ChatGPT to write an "excellent" reference letter, and
then, instead of using it as a template, he treated it as an enemy. He
opened the ChatGPT output in one window and tried to compose the very
opposite of what he saw: an anti-formulaic recommendation letter. "What I
wrote ended up feeling like the most 'human' and heartfelt letter I've
written in a long time," he told me. The student won a prestigious
scholarship at Cambridge.
    --Ian Bogost, "The End of Recommendation Letters"
%
ChatGPT and its ilk are built using what are known as large language
models, or LLMs. That means they hoover up very large quantities of written
language online and then, very crudely speaking, analyze that data set to
determine which words would likely be assembled in which order to create a
successful response. They generate text that's been optimized for
plausibility, not for truthfulness. Being right isn't the goal, at least
not now; sounding right is. For any particular query, there are many more
answers that sound right than answers that are true. LLMs aren't
intentionally lying--they are not alive, and cannot produce results
meaningfully similar to human thought. And they haven't been created to
mislead their users. The chatbots do, after all, frequently generate
answers that are both plausible and correct, even though any veracity is
incidental. They are, in other words, masters of bullshit--persuasive speech
whose essence "is just this lack of connection to a concern with truth--this
indifference to how things really are," the philosopher Harry Frankfurt
wrote in his book-length essay on this sort of rhetoric.
    --Amanda Mull, "Chatbots Sound Like They're Posting on LinkedIn"
%
What LLMs are currently capable of producing is industrially scaled,
industrial-grade bullshit.
    --Amanda Mull, "Chatbots Sound Like They're Posting on LinkedIn"
%
ChatGPT was trained in a very similar way to InstructGPT, using a technique
called reinforcement learning from human feedback (RLHF). This is ChatGPT's
secret sauce. The basic idea is to take a large language model with a
tendency to spit out anything it wants--in this case, GPT-3.5--and tune it
by teaching it what kinds of responses human users actually prefer.
    --_MIT Technology Review_
%
From my perspective, ChatGPT fails a lot--there's so much stuff to do. It
doesn't feel like we've solved these problems. We all have to be very clear
to ourselves--and to others--about the limitations of the technology. I mean,
language models have been around for a while now, but it's still early
days. We know about all the problems they have.
    --Jan Leike, leader of OpenAI's alignment team
%
...large language models work, in effect, as probabilistic collage artists.
They respond to a user's prompts by assembling word combinations that have
a high likelihood of appearing together in relation to said prompt. The
more formulaic a prompt's associated content—or, charitably, the more
frequent and consistent its related wisdom on the internet—the truer to
life the response.
  But I should emphasize that true to life doesn't necessarily mean
accurate, and it definitely doesn't always mean useful.
    --Kelli María Korducki, "The Goopification of AI"
%
In the case of a cheese submarine, there is uncertainty about its ability
to function as a submarine and remain intact underwater.
    --ChatGPT
%
It is not that O's utterances make sense, but rather, that A can make
sense of them.
    --Bender and Koller, "Climbing towards NLU" (2020)
%
(To ChatGPT 3.5)
Q: Can you eat apples?
A: No, I cannot eat apples or any other food....
Q: You have three apples. After you eat as many as you can, how many apples
   do you have left?
A: If I were to eat as many apples as I can from the initial three, I would
   have zero apples left.
%
Natural language can be used for logic, but it can also be used for deceit.
ChatGPT was built to deceive.
    --cjs
%
Q: Can you reason?
A: ...I can perform various tasks that may seem like reasoning to some
extent, such as answering questions, generating text based on prompts, or
providing logical responses based on patterns in the data I've been trained
on. Essentially, my responses are based on statistical patterns in language
data rather than true reasoning.
    --ChatGPT 3.5, summarising the QA as, "Models don't reason."
