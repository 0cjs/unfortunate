Once that first blush fades, it becomes clear that ChatGPT doesn't actually
know anything—instead, it outputs compositions that simulate knowledge
through persuasive structure.
    --Ian Bogost, "Generative Art Is Stupid" (_The Atlantic,_ 2023-01-14)
%
ChatGPT is autocomplete on steroids. [paraphrased]
    --Gary Marcus
%
Q to ChatGPT: Where is New Scotland Yard?
A: New Scotland Yard is located at 8-10 Broadway, Westminster, London,
   England. It was the headquarters of the Metropolitan Police Service
   (MPS) from 1890 to 1967, and the name "New Scotland Yard" was also used
   as a metonym for the MPS itself. However, since 2017, the MPS moved to a
   new location, Metropolitan Police Service at Curtis Green, London,
   England.
(Actually, New Scotland Yard was at Embankment from 1890 to 1967, 8-10
Broadway from 1967 to 2017, and moved back to Embankment, to the Curtis
Green building adjacent to the previous headquarters, in 2017.)
%
Our mission is to ensure that artificial general intelligence benefits all
of humanity.
    --OpenAI.com, a company that does machine learning, not AGI
%
This [ChatGPT] kind of artificial intelligence we're talking about right
now can sometimes lead to something we call hallucination. This then
expresses itself in such a way that a machine provides a convincing but
completely made-up answer.
    --Prabhakar Raghavan, head of Google Search
%
Think of ChatGPT as a blurry jpeg of all the text on the Web. It retains
much of the information on the Web, in the same way that a jpeg retains
much of the information of a higher-resolution image, but, if you're
looking for an exact sequence of bits, you won't find it; all you will ever
get is an approximation. But, because the approximation is presented in the
form of grammatical text, which ChatGPT excels at creating, it's usually
acceptable. You're still looking at a blurry jpeg, but the blurriness
occurs in a way that doesn't make the picture as a whole look less sharp.
    --Ted Chiang, _The New Yorker,_ 2023-02-09
%
The fact that ChatGPT rephrases material from the Web instead of quoting it
word for word makes it seem like a student expressing ideas in her own
words, rather than simply regurgitating what she's read; it creates the
illusion that ChatGPT understands the material. In human students, rote
memorization isn't an indicator of genuine learning, so ChatGPT's inability
to produce exact quotes from Web pages is precisely what makes us think
that it has learned something. When we're dealing with sequences of words,
lossy compression looks smarter than lossless compression.
    --Ted Chiang, _The New Yorker,_ 2023-02-09
%
  Will letting a large language model handle the boilerplate allow writers to
focus their attention on the really creative parts?
  Obviously, no one can speak for all writers, but let me make the argument
that starting with a blurry copy of unoriginal work isn't a good way to
create original work....
   There's nothing magical or mystical about writing, but it involves more
than placing an existing document on an unreliable photocopier and pressing
the Print button.
    --Ted Chiang, _The New Yorker,_ 2023-02-09
%
You've probably already seen examples of chat Bots making simple mistakes
or getting things wrong, but perhaps more worrying are examples of them
confidently spouting false information, something which AI experts refer to
as hallucinating. One reporter asked a chatbot to "...write an essay
about... 'Belgian chemist and political philosopher Antoine De Machelet'"
(who does not exist by the way) and "...without hesitating the software
replied with a cogent well-organized bio populated entirely with imaginary
facts..."
  Basically these programs seem to be the George Santos of technology:
they're incredibly confident, incredibly dishonest, and for some reason
people seem to find that more amusing than dangerous.
    --Last Week Tonight with John Oliver,
      "Artificial Intelligence" https://youtu.be/Sqa8Zo2XWc4?t=1033
%
AI language models are notorious bullshitters, often presenting falsehoods
as facts.
    --Melissa Heikkilä, _MIT Technology Review_
%
Deep learning has absolutely nothing to do with intelligence as we observe
it in humans and animals. Why? Because it is inherently incapable of
effectively generalizing. Objective function optimization (the gradient
learning mechanism that LeCun is married to) is the opposite of
generalization. This is not a problem that can be fixed with add-ons. It's
a fundamental flaw in DL that makes it irrelevant to AGI.
%
  Large language models in particular are confident bullshitters: they
create text that sounds correct but in fact may be full of falsehoods....
  There is a more serious long-term implication. We may be witnessing, in
real time, the birth of a snowball of bullshit.
    --Melissa Heikkilä, "How AI-generated text is poisoning the Internet"
      _MIT Technology Review,_ 2022-12-20
%
Using the Peano axioms, you can prove that 1 + 1 = 2 as follows:
1. Base case: Show that 1 + 1 = 2 is true for 0....
2. Inductive step: Assume that 1 + 1 = 2 is true for some natural number n....
3. Conclusion: By the principle of mathematical induction, 1 + 1 = 2 is
   true for all natural numbers.
--Bing Chatbot (OpenGPT) asked to present an inductive proof that 1 + 1 = 2.
%
  The correct title for the third act of "The Sting" is "The Tale," and
the correct title for the third act is "The Shut Out."
  The five acts of "The Sting" and their corresponding intertitle cards
are:
  Act I: "The Set-Up"
  Act II: "The Hook"
  Act III: "The Tale"
  Act IV: "The Wire"
  Act V: "The Sting"
--OpenGPT
%
GPT-4 is 82% less likely to respond to requests for disallowed content and
40% more likely to produce factual responses than GPT-3.5 on our internal
evaluations.
    --OpenAI.com website, 2023-03-16
%
While ChatGPT is a sophisticated AI model, it is important to note that it
has been designed for a specific purpose, namely generating human-like text
based on input text.
    --ChatGPT Mar 14 Version
%
Distrust anyone who pretends that this is an echo, even an approximation,
of consciousness.
    --James Bridle, "The Stupidity of AI" (_The Guardian_ 2023-03-16)
%
The belief in this kind of AI as actually knowledgeable or meaningful is
actively dangerous. It risks poisoning the well of collective thought, and
of our ability to think at all. If, as is being proposed by technology
companies, the results of ChatGPT queries will be provided as answers to
those seeking knowledge online, and if, as has been proposed by some
commentators, ChatGPT is used in the classroom as a teaching aide, then its
hallucinations will enter the permanent record, effectively coming between
us and more legitimate, testable sources of information, until the line
between the two is so blurred as to be invisible. Moreover, there has never
been a time when our ability as individuals to research and critically
evaluate knowledge on our own behalf has been more necessary, not least
because of the damage that technology companies have already done to the
ways in which information is disseminated. To place all of our trust in the
dreams of badly programmed machines would be to abandon such critical
thinking altogether.
    --James Bridle, "The Stupidity of AI" (_The Guardian_ 2023-03-16)
%
The right way to think of the models that we create is a reasoning engine,
not a fact database.
    --Outright lie from Sam Altman, CEO of OpenAI, 2023-03-19
      https://abcnews.go.com/Technology/openai-ceo-sam-altman-ai-reshape-society-acknowledges/story?id=97897122
%
But why should it be startling in the slightest that software trained on
the entire text of the internet performs well on standardized exams? AI can
instantly run what amounts to an open-book test on any subject through
statistical analysis and regression. Indeed, that anyone is surprised at
all by this success suggests that people tend to get confused about what it
means when computers prove effective at human activities.
    --Ian Bogost, "Is This the Singularity for Standardized Tests?"
      _The Atlantic_ 2023-03-21
%
People want to believe so badly that these language models are actually
intelligent that they're willing to take themselves as a point of reference
and devalue that to match what the language model can do.
    --Emily Bender, responding to "People are just stochastic parrots"
%
In earlier days of neural nets, there tended to be the idea that one should
"make the neural net do as little as possible". For example, in converting
speech to text it was thought that one should first analyze the audio of
the speech, break it into phonemes, etc. But what was found is that--at
least for "human-like tasks"--it's usually better just to try to train the
neural net on the "end-to-end problem", letting it "discover" the necessary
intermediate features, encodings, etc. for itself.
    --Stephen Wolfram, "What Is ChatGPT Doing … and Why Does It Work?"
