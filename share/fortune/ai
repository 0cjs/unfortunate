Once that first blush fades, it becomes clear that ChatGPT doesn’t actually
know anything—instead, it outputs compositions that simulate knowledge
through persuasive structure.
    --Ian Bogost, "Generative Art Is Stupid" (_The Atlantic,_ 2023-01-14)
%
ChatGPT is autocomplete on steroids. [paraphrased]
    --Gary Marcus
%
Q to ChatGPT: Where is New Scotland Yard?
A: New Scotland Yard is located at 8-10 Broadway, Westminster, London,
   England. It was the headquarters of the Metropolitan Police Service
   (MPS) from 1890 to 1967, and the name "New Scotland Yard" was also used
   as a metonym for the MPS itself. However, since 2017, the MPS moved to a
   new location, Metropolitan Police Service at Curtis Green, London,
   England.
(Actually, New Scotland Yard was at Embankment from 1890 to 1967, 8-10
Broadway from 1967 to 2017, and moved back to Embankment, to the Curtis
Green building adjacent to the previous headquarters, in 2017.)
%
Our mission is to ensure that artificial general intelligence benefits all
of humanity.
    --OpenAI.com, a company that does machine learning, not AGI
%
This [ChatGPT] kind of artificial intelligence we're talking about right
now can sometimes lead to something we call hallucination. This then
expresses itself in such a way that a machine provides a convincing but
completely made-up answer.
    --Prabhakar Raghavan, head of Google Search
%
Think of ChatGPT as a blurry jpeg of all the text on the Web. It retains
much of the information on the Web, in the same way that a jpeg retains
much of the information of a higher-resolution image, but, if you’re
looking for an exact sequence of bits, you won’t find it; all you will ever
get is an approximation. But, because the approximation is presented in the
form of grammatical text, which ChatGPT excels at creating, it’s usually
acceptable. You’re still looking at a blurry jpeg, but the blurriness
occurs in a way that doesn’t make the picture as a whole look less sharp.
    --Ted Chiang, _The New Yorker,_ 2023-02-09
%
The fact that ChatGPT rephrases material from the Web instead of quoting it
word for word makes it seem like a student expressing ideas in her own
words, rather than simply regurgitating what she’s read; it creates the
illusion that ChatGPT understands the material. In human students, rote
memorization isn’t an indicator of genuine learning, so ChatGPT’s inability
to produce exact quotes from Web pages is precisely what makes us think
that it has learned something. When we’re dealing with sequences of words,
lossy compression looks smarter than lossless compression.
    --Ted Chiang, _The New Yorker,_ 2023-02-09
%
  Will letting a large language model handle the boilerplate allow writers to
focus their attention on the really creative parts?
  Obviously, no one can speak for all writers, but let me make the argument
that starting with a blurry copy of unoriginal work isn’t a good way to
create original work....
   There’s nothing magical or mystical about writing, but it involves more
than placing an existing document on an unreliable photocopier and pressing
the Print button.
    --Ted Chiang, _The New Yorker,_ 2023-02-09
%
You've probably already seen examples of chat Bots making simple mistakes
or getting things wrong, but perhaps more worrying are examples of them
confidently spouting false information, something which AI experts refer to
as hallucinating. One reporter asked a chatbot to "...write an essay
about... 'Belgian chemist and political philosopher Antoine De Machelet'"
(who does not exist by the way) and "...without hesitating the software
replied with a cogent well-organized bio populated entirely with imaginary
facts..."
  Basically these programs seem to be the George Santos of technology:
they're incredibly confident, incredibly dishonest, and for some reason
people seem to find that more amusing than dangerous.
    --Last Week Tonight with John Oliver,
      "Artificial Intelligence" https://youtu.be/Sqa8Zo2XWc4?t=1033
%
AI language models are notorious bullshitters, often presenting falsehoods
as facts.
    --Melissa Heikkilä, _MIT Technology Review_
%
Deep learning has absolutely nothing to do with intelligence as we observe
it in humans and animals. Why? Because it is inherently incapable of
effectively generalizing. Objective function optimization (the gradient
learning mechanism that LeCun is married to) is the opposite of
generalization. This is not a problem that can be fixed with add-ons. It's
a fundamental flaw in DL that makes it irrelevant to AGI.
%
  Large language models in particular are confident bullshitters: they
create text that sounds correct but in fact may be full of falsehoods....
  There is a more serious long-term implication. We may be witnessing, in
real time, the birth of a snowball of bullshit.
    --Melissa Heikkilä, "How AI-generated text is poisoning the Internet"
      _MIT Technology Review,_ 2022-12-20
