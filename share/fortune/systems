It turned out that their employees followed nearly every safety
regulation nearly all the time, with a few exceptions. One exception
was when the rig was down—not operating. When a rig is up, it brings
in about a quarter of a million dollars a day. When it's down, it
brings in nothing, and as you might imagine, everyone goes into
emergency overdrive. The sound of “We're not making money!” rings
through the rig, and people respond by taking shortcuts and skipping
safety steps in order to get the rig back to making money. Nobody asks
them to take shortcuts or turn a blind eye to safety protocols. They
just do.
    --_Influencer: The New Science of Leading Change_ 2nd ed., Ch.4
%
Most of us have our favorite influence methods—just pass a law, just
threaten a consequence, or just offer a training program. The problem
with sticking to our favorite methods is not that the methods are
flawed per se; it's that they are far too simplistic. It's akin to
hiking the Himalayas with only a bag lunch. There's nothing wrong with
Gatorade and a granola bar, but you'll probably need a lot more.
Bringing a simple solution to a complex behavioral challenge almost
never works.
    --_Influencer: The New Science of Leading Change_ 2nd ed., Pt.2
%
The complexity of these systems makes it impossible for them to run without
multiple flaws being present....
  After accident reviews nearly always note that the system has a history of
prior 'proto-accidents' that nearly generated catastrophe. Arguments that
these degraded conditions should have been recognized before the overt
accident are usually predicated on naïve notions of system performance.
System operations are dynamic, with components (organizational, human,
technical) failing and being replaced continuously.
    --Richard I. Cook, "How Complex Systems Fail"
      https://how.complexsystems.fail/
%
7. Post-accident attribution to a 'root cause' is fundamentally wrong.
   Because overt failure requires multiple faults, there is no isolated
'cause' of an accident. There are multiple contributors to accidents. Each of
these is necessarily insufficient in itself to create an accident. Only
jointly are these causes sufficient to create an accident. Indeed, it is the
linking of these causes together that creates the circumstances required for
the accident. Thus, no isolation of the 'root cause' of an accident is
possible. The evaluations based on such reasoning as 'root cause' do not
reflect a technical understanding of the nature of failure but rather the
social, cultural need to blame specific, localized forces or events for
outcomes.
    --Richard I. Cook, "How Complex Systems Fail"
      https://how.complexsystems.fail/
%
..._ex post facto_ accident analysis of human performance is inaccurate. The
outcome knowledge poisons the ability of after-accident observers to recreate
the view of practitioners before the accident of those same factors....
_Hindsight bias remains the primary obstacle to accident investigation,
especially when expert human performance is involved._
    --Richard I. Cook, "How Complex Systems Fail"
      https://how.complexsystems.fail/
%
10. All practitioner actions are gambles.
    After accidents, the overt failure often appears to have been inevitable
and the practitioner’s actions as blunders or deliberate willful disregard of
certain impending failure. But all practitioner actions are actually gambles,
that is, acts that take place in the face of uncertain outcomes. The degree
of uncertainty may change from moment to moment. That practitioner actions
are gambles appears clear after accidents; in general, post hoc analysis
regards these gambles as poor ones. But the converse: that successful
outcomes are also the result of gambles; is not widely appreciated.
    --Richard I. Cook, "How Complex Systems Fail"
      https://how.complexsystems.fail/
%
15. Views of ‘cause’ limit the effectiveness of defenses against future events.
  Post-accident remedies for “human error” are usually predicated on
obstructing activities that can “cause” accidents. These end-of-the-chain
measures do little to reduce the likelihood of further accidents. In fact
that likelihood of an identical accident is already extraordinarily low
because the pattern of latent failures changes constantly. Instead of
increasing safety, post-accident remedies usually increase the coupling and
complexity of the system. This increases the potential number of latent
failures and also makes the detection and blocking of accident trajectories
more difficult.
    --Richard I. Cook, "How Complex Systems Fail"
      https://how.complexsystems.fail/
%
[Systems] promise to do a hard job faster, better, and more easily than you
could do it by yourself. But if you set up a system, you are likely to find
your time and effort now being consumed in the care and feeding of the
system itself. New problems are created by its very presence. Once set up,
it won't go away, it grows and encroaches. It begins to do strange and
wonderful things. Breaks down in ways you never thought possible. It kicks
back, gets in the way, and opposes its own proper function. Your own
perspective becomes distorted by being in the system. You become anxious
and push on it to make it work. Eventually you come to believe that the
misbegotten product it so grudgingly delivers is what you really wanted all
the time. At that point encroachment has become complete. You have become
absorbed. You are now a systems person.
    --John Gall, _General Systemantics_ (1975)
